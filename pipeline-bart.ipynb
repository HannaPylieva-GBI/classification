{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634dce75-2a81-46bc-87ce-842a45629023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a1fcdb-e07e-4328-aff5-b12111259777",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = shell_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881ea9e4-4755-4154-94b8-54b0b7ac87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'groupby-development'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec7dc74-6a0e-43b0-918f-a71737f026f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://gbi_ml/classification_hackathon/pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941980f5-7417-4d6e-bc04-a3c6069f55f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://gbi_ml/classification_hackathon/pipelines'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08d0eb3-f306-41f7-98b6-66dc5f47b59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: Incorrect option(s) specified. Usage:\n",
      "\n",
      "  gsutil mb [-b (on|off)] [-c <class>] [-k <key>] [-l <location>] [-p <project>]\n",
      "            [--autoclass] [--retention <time>] [--pap <setting>]\n",
      "            [--rpo (ASYNC_TURBO|DEFAULT)] gs://<bucket_name>...\n",
      "\n",
      "For additional help run:\n",
      "  gsutil help mb\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43be1d47-aac5-4caa-b870-65936679cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a64a243d-19a8-4434-8bbb-764867ec385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 937725678441-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].strip()[8:]\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64122fe2-f3e3-4c61-b4ec-5b9626a52350",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4a54f8-d6f8-41c2-8b4f-0f6103599357",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "VCPU = \"16\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3ac7c7-1213-4d3d-aeae-f43cec36723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc9de9a-f0fe-46ee-8329-14f083ce95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = 'classification_hackathon_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5307cb7-dfb2-491e-80c7-4b2f11ced7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470f174-a268-43ad-afe4-2f1fa645ac69",
   "metadata": {},
   "source": [
    "## Model train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fddff32d-7f3c-48dd-8802-863921fb08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from pathlib import Path\n",
    "\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10104216-3db0-416a-88f2-d4257025efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training\n",
    "@dsl.component(\n",
    "    base_image='huggingface/transformers-pytorch-gpu:4.23.1',\n",
    "    packages_to_install=['fsspec==2022.11.0', \n",
    "                         'gcsfs==2022.11.0', \n",
    "                         'google-cloud-secret-manager', \n",
    "                         'google-cloud-bigquery',\n",
    "                         'google-cloud-bigquery-storage',\n",
    "                         'evaluate',\n",
    "                         'pandas==1.5.2']\n",
    ")\n",
    "def custom_training(\n",
    "    pretrained_model: str,\n",
    "    model_dir: str,\n",
    "    learning_rate: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"Custom training for Huggingface Sequence Classification models. \n",
    "    This function is based on the Huggingface Trainer class, and allow any Huggingface \n",
    "    pretrained models supported for sequence classification.\n",
    "\n",
    "    Args:\n",
    "        pretrained_model: Huggingface model name.\n",
    "        model_output: GSC path to save model artifacts.\n",
    "        label_mapping: GCS path to label mapping, has to be in JSON format.\n",
    "        learning_rate: Learning rate for training.\n",
    "        epochs: Number of epochs for training.\n",
    "        batch_size: Batch size for training.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import gcsfs\n",
    "    import pickle\n",
    "    import logging\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import evaluate\n",
    "    from datasets import Dataset, DatasetDict\n",
    "    from transformers import (DataCollatorWithPadding, AutoModelForSequenceClassification,\n",
    "                              TrainingArguments, AutoTokenizer, Trainer,\n",
    "                              EarlyStoppingCallback, IntervalStrategy\n",
    "                              )\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Use accuracy as metrics for now, can add more sophistication to compute_metrics later\n",
    "    target_metric = evaluate.load(\"accuracy\")\n",
    "    PROJECT_ID = 'groupby-development'\n",
    "    CHECKPOINT = pretrained_model\n",
    "    \n",
    "    def create_datasets(ds, tokenizer):\n",
    "        '''Creates a tf.data.Dataset for train and evaluation.'''\n",
    "        tokenized_datasets = ds.map((lambda examples: tokenize_function(examples, tokenizer)), batched=True)\n",
    "\n",
    "        # To speed up training, we use only a portion of the data.\n",
    "        # Use full_train_dataset and full_eval_dataset if you want to train on all the data.\n",
    "        return tokenized_datasets['train'], tokenized_datasets['validation']\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return target_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    def tokenize_function(examples, tokenizer):\n",
    "        '''Tokenizes text examples.'''\n",
    "\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "    \n",
    "    def main():\n",
    "        fs = gcsfs.GCSFileSystem(project=PROJECT_ID)\n",
    "        \n",
    "        # Prepare label mapping\n",
    "        label_mapping='gs://gbi_ml/classification_hackathon/label2id.pickle'\n",
    "        logging.info(f'Label mapping path: {label_mapping}')\n",
    "        \n",
    "        with fs.open(label_mapping, 'rb') as handle:\n",
    "            label2id = pickle.load(handle)\n",
    "        \n",
    "        id_mapping='gs://gbi_ml/classification_hackathon/id2label.pickle'\n",
    "        with fs.open(id_mapping, 'rb') as handle:\n",
    "            id2label = pickle.load(handle)\n",
    "        \n",
    "        from collections import defaultdict\n",
    "  \n",
    "        def def_value():\n",
    "            return -1\n",
    "\n",
    "        # Defining the dict\n",
    "        d = defaultdict(int, label2id)\n",
    "        \n",
    "        # Prepare model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CHECKPOINT, id2label=id2label, label2id=label2id)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "        \n",
    "        train_set = 'gs://gbi_ml/classification_hackathon/bbby_train_new.csv'\n",
    "        test_set = 'gs://gbi_ml/classification_hackathon/bbby_test_new.csv'\n",
    "        # Prepare datasets\n",
    "        with fs.open(train_set, \"r\") as f:\n",
    "            logging.info(f'Train set path: {train_set}')\n",
    "            train_df = pd.read_csv(f)[[\"raw_product_description\", \"bucket_name\"]]\n",
    "            train_df.rename(columns={'raw_product_description':'text'}, inplace=True)\n",
    "            train_df = train_df[train_df[\"text\"].notna()]\n",
    "            train_df['label'] = train_df.bucket_name.apply(lambda x: label2id[x])\n",
    "        with fs.open(test_set, \"r\") as f:\n",
    "            logging.info(f'Validation set path: {test_set}')\n",
    "            val_df = pd.read_csv(f)[[\"raw_product_description\", 'Manual Classification Bucket']]\n",
    "            val_df.rename(columns={'raw_product_description':'text'}, inplace=True)\n",
    "            val_df = val_df[val_df[\"text\"].notna()]\n",
    "            val_df['label'] = val_df['Manual Classification Bucket'].apply(lambda x: d[x])\n",
    "\n",
    "        # Convert labels to ids for training purposes\n",
    "        # train_df[\"label\"] = train_df[\"label\"].apply(lambda x: label2id[x])\n",
    "        # val_df[\"label\"] = val_df[\"label\"].apply(lambda x: label2id[x])\n",
    "        # logging.info(\"Label mapping: {}\".format(id2label))\n",
    "        logging.info(\"Train set: {}\".format(train_df.shape))\n",
    "        logging.info(\"Validation set: {}\".format(val_df.shape))\n",
    "        \n",
    "        # Prepare Huggingface Datasets\n",
    "        tds = Dataset.from_pandas(train_df)\n",
    "        vds = Dataset.from_pandas(val_df)\n",
    "        ds = DatasetDict()\n",
    "\n",
    "        ds['train'] = tds\n",
    "        ds['validation'] = vds\n",
    "        \n",
    "        # Create train and validation set for tensorflow training\n",
    "        train_ds, val_ds = create_datasets(ds, tokenizer)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "        # Model training configuration\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./model_output',\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            log_level=\"debug\",\n",
    "            logging_strategy=\"steps\",\n",
    "            save_total_limit=3,\n",
    "            eval_steps=100,\n",
    "            save_steps=500,\n",
    "            seed=42,\n",
    "            save_strategy=\"steps\",\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.02)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model locally first, then copy model artifacts direct\n",
    "        trainer.save_model('./model_output')\n",
    "        fs.put('./model_output/', model_dir, recursive=True)\n",
    "        test_metric = trainer.evaluate(val_ds)\n",
    "        print(test_metric)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e913d5be-5574-4630-a34a-2ad59ecaf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_training_op = utils.create_custom_training_job_op_from_component(\n",
    "    custom_training, \n",
    "    machine_type='n1-standard-8',\n",
    "    accelerator_type=\"NVIDIA_TESLA_P100\",\n",
    "    accelerator_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1e1da87d-1e29-4d3c-b10c-d32d251b92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "41bcdb1c-b8bf-4a0b-9ae1-8f234d6d7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"facebook/bart-large\"\n",
    "DISPLAY_NAME=f'huggingface-sequence-classification-{PRETRAINED_MODEL}-{TIMESTAMP}'\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "MODEL_DIR = f\"{PIPELINE_ROOT}/model_artifacts\"\n",
    "MODEL_ARTIFACTS_LOCAL = \"./model_artifacts\"\n",
    "MODEL_NAME = f\"huggingface-sequence-classification-{PRETRAINED_MODEL}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec35f57e-47c9-4827-9261-7926395284f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"huggingface-classification-training-pipeline\")\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    root_dir: str = PIPELINE_ROOT,\n",
    "    pretrained_model: str = PRETRAINED_MODEL,\n",
    "    learning_rate: float = LEARNING_RATE,\n",
    "    epochs: int = EPOCHS,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    model_dir: str = MODEL_DIR,\n",
    "):\n",
    "    custom_training_task = custom_training_op(\n",
    "        project=project_id,\n",
    "        location=REGION,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        pretrained_model=pretrained_model,\n",
    "        model_dir=model_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2e9df85-9126-4e98-ac94-1a288b8337f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1295: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/huggingface-classification-training-pipeline-20230323095028?project=937725678441\n",
      "PipelineJob projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/937725678441/locations/us-central1/pipelineJobs/huggingface-classification-training-pipeline-20230323095028 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [custom-training].; Job (project_id = groupby-development, job_id = 4272698337246838784) is failed due to the above error.; Failed to handle the job: {project_number = 937725678441, job_id = 4272698337246838784}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27128/3144444495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSERVICE_ACCOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    349\u001b[0m         )\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     def submit(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [custom-training].; Job (project_id = groupby-development, job_id = 4272698337246838784) is failed due to the above error.; Failed to handle the job: {project_number = 937725678441, job_id = 4272698337246838784}\"\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"classification_training_spec.json\"\n",
    ")\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"classification_training_spec.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccfad0-9353-47d3-8b29-9e04e353d588",
   "metadata": {},
   "source": [
    "## RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 15.90 GiB total capacity; 11.62 GiB already allocated; 1.84 GiB free; 13.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "\n",
    "need either train on several GPUs of use a GPU with more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e0662-d978-4839-8595-590d02bb8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction example\n",
    "# Encode the text\n",
    "encoded = tokenizer(input_texts, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Call the model to predict under the format of logits of 27 classes\n",
    "logits = model(**encoded).logits.cpu().detach().numpy()\n",
    "\n",
    "# Decode the result\n",
    "preds = get_preds_from_logits(logits)\n",
    "decoded_preds = [[id2label[i] for i, l in enumerate(row) if l == 1] for row in preds]\n",
    "\n",
    "for text, pred in zip(input_texts, decoded_preds):\n",
    "    print(text)\n",
    "    print(\"Global feeling:\", [LABEL_DICTIONARY[l] for l in pred if l.startswith(\"S\")])\n",
    "    print(\"Emotions:\", [LABEL_DICTIONARY[l] for l in pred if l.startswith(\"E\")])\n",
    "    print(\"Causes:\", [LABEL_DICTIONARY[l] for l in pred if l.startswith(\"C\") and l != \"C9\"]) # Exclude \"no cause\" for simpler reading\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf-gpu.1-15.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
